{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"jasoseo_classifications_pytorch_kobert_final.ipynb","provenance":[{"file_id":"https://github.com/SKTBrain/KoBERT/blob/master/scripts/NSMC/naver_review_classifications_pytorch_kobert.ipynb","timestamp":1629705446408}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5baa7ea8994c4240b9a96f2235790b26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a7456d9a9a043eaaa94f5795f563852","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b6b582a5a74341388f01888f8439778f","IPY_MODEL_304939764b9b482cb198dbe1e5231545","IPY_MODEL_3263bd652d13449aa4a0f5366f2dee3c"]}},"7a7456d9a9a043eaaa94f5795f563852":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b6b582a5a74341388f01888f8439778f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_552e81718bb744ff9a5eb5209a30f386","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 68%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba0ab3f2167947b590e1802931a96589"}},"304939764b9b482cb198dbe1e5231545":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ecfae73b6eb544b4b858229862dc729e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":32567,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":22070,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4beda4125333499fa8650377c03ada16"}},"3263bd652d13449aa4a0f5366f2dee3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fd02dbd67f4641ae95f73fe744a9549b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 22070/32567 [2:48:18&lt;1:20:20,  2.18it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9454470b040d46539a6a1d0b221a1214"}},"552e81718bb744ff9a5eb5209a30f386":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ba0ab3f2167947b590e1802931a96589":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ecfae73b6eb544b4b858229862dc729e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4beda4125333499fa8650377c03ada16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd02dbd67f4641ae95f73fe744a9549b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9454470b040d46539a6a1d0b221a1214":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"hK06EUKFCu30"},"source":["import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z9O13IvBwdw","executionInfo":{"status":"ok","timestamp":1629942513841,"user_tz":-540,"elapsed":29648,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"7c77304e-018f-46bb-bd90-b9f89ad3a2be"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":566},"id":"Oe_24n-wBQnm","scrolled":true,"executionInfo":{"status":"ok","timestamp":1629942520132,"user_tz":-540,"elapsed":4142,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"0f4131b8-558f-4a02-bcf8-59536b5eb1d7"},"source":["df = pd.read_csv('/content/gdrive/MyDrive/데청캠 3조 공유 드라이브/Model/성민/data/sentencePairData.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>first_sentence</th>\n","      <th>second_sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>둘째, ICT 산학협력 SW 교에서는 오답 노트 시스템을 개발했습니다.</td>\n","      <td>네 좋아해요</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>금융업계에서의 올해 최고 이슈는 ‘핀테크’입니다.</td>\n","      <td>은행을 비롯해 모든 금융업계는 IT인프라를 늘리고, 관련 부서를 신설하는 등 핀테크...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>이러한 열정과 도전정신은 저의 인생 모토가 되었고 어떠한 위기와 역경에도 일단 부딪...</td>\n","      <td>학생회를 통해 리더십과 책임감을 발휘할 수 있었으며 혼자가 아닌 팀으로 하여 더 효...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>업무일지 작성으로 사내 인력과 상황에의 대처능력과 조직적응력을 빠르게 나타내겠습니다.</td>\n","      <td>선수들이 거세게 항의를 하면 판정을 번복하기도 하고, 스로인 반칙을 보고도 그냥 넘...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>카드는 여행 구성에 상관없이 반드시 쓰는 혜택을 구성하고 O2O 서비스를 통해 고객...</td>\n","      <td>신한 판클럽은 야 놀자 하고 제휴는 맺었지만 여행지나 렌터카 등 부가적인 서비스는 ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>325657</th>\n","      <td>325657</td>\n","      <td>아직 배울 게 많은 신입사원입니다.</td>\n","      <td>친구 결혼식도 갈 겸 회사 인턴도 할 겸 맞추려고 하거든요</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>325658</th>\n","      <td>325658</td>\n","      <td>제가 소극적이였을 때는 할 수 있는 것도 못하고 피해를 보는 경우가 있었지만 지금은...</td>\n","      <td>주변 상가는 장사가 잘 되나요?</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>325659</th>\n","      <td>325659</td>\n","      <td>더불어 같은 조원이였던 중국학우를 통해 다른 나라 문화의 도시계획에 대해 깊은 인상...</td>\n","      <td>네 데워드려요</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>325660</th>\n","      <td>325660</td>\n","      <td>이론적인 내용에만 만족하는 것이 아닌 직접 제작하며 배운 회로는 이론과 너무나 달랐...</td>\n","      <td>시설관리업무는 처음이라 ‘할 수 있는 것’이라는 확신도 없습니다.</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>325661</th>\n","      <td>325661</td>\n","      <td>다음으로 섬유제조업체에서의 최고뿐만 아니라, 패션브랜드업체로서의 입지를 다지고 싶습니다.</td>\n","      <td>이를 위해 ‘모비토’와 같은 다다C&amp;C의 새로운 패션브랜드들을 런칭하여 패션시장에서...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>325662 rows × 4 columns</p>\n","</div>"],"text/plain":["        Unnamed: 0  ... label\n","0                0  ...     0\n","1                1  ...     1\n","2                2  ...     0\n","3                3  ...     0\n","4                4  ...     1\n","...            ...  ...   ...\n","325657      325657  ...     0\n","325658      325658  ...     0\n","325659      325659  ...     0\n","325660      325660  ...     0\n","325661      325661  ...     1\n","\n","[325662 rows x 4 columns]"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"GvCpaEw9EKOZ"},"source":["data_set = list()\n","for i in range(0, len(df)):\n","    data_set.append([df['first_sentence'][i],df['second_sentence'][i], df['label'][i]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lh-rx_npEmbM","executionInfo":{"status":"ok","timestamp":1629942525712,"user_tz":-540,"elapsed":23,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"5bcac0d7-06a8-4813-d36b-3dd3bcfe77ff"},"source":["test_per = 0.2\n","train_size = int(len(data_set)*(1-test_per))\n","print(train_size) \n","train_data = data_set[:train_size]\n","test_data = data_set[train_size: len(data_set)]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["260529\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4PwqvE_x_Zhc","executionInfo":{"status":"ok","timestamp":1629942528227,"user_tz":-540,"elapsed":2535,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"e1ea337d-5223-420d-9e0c-b0a21f6fd177"},"source":["import torch\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB'"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nzdbCXyA-IW","executionInfo":{"status":"ok","timestamp":1629942552986,"user_tz":-540,"elapsed":24774,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"abc0343a-767f-4257-dba1-d4011c995d78"},"source":["!pip install mxnet\n","!pip install gluonnlp pandas tqdm\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting mxnet\n","  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n","\u001b[K     |████████████████████████████████| 46.9 MB 69 kB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[K     |████████████████████████████████| 344 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (1.19.5)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (0.29.24)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp) (21.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp) (2.4.7)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp37-cp37m-linux_x86_64.whl size=595735 sha256=c5aa8712190d11f55759662763964b760eb0bb6824f6cf27444c980f0efe7892\n","  Stored in directory: /root/.cache/pip/wheels/be/b4/06/7f3fdfaf707e6b5e98b79c041e023acffbe395d78a527eae00\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Collecting transformers\n","  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 55.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 41.5 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 64.7 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76qBmkqit3vC","executionInfo":{"status":"ok","timestamp":1629942557650,"user_tz":-540,"elapsed":4699,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"e2310902-31ca-4305-f282-0bded85c7e11"},"source":["!pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://****@github.com/SKTBrain/KoBERT.git@master\n","  Cloning https://****@github.com/SKTBrain/KoBERT.git (to revision master) to /tmp/pip-req-build-700w41kc\n","  Running command git clone -q 'https://****@github.com/SKTBrain/KoBERT.git' /tmp/pip-req-build-700w41kc\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.1.2-py3-none-any.whl size=12771 sha256=46cab82b7782a76f788f1fc16d6df01b80a5cbe6b253b21a6f1f53a26b734294\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4uogan16/wheels/d3/68/ca/334747dfb038313b49cf71f84832a33372f3470d9ddfd051c0\n","Successfully built kobert\n","Installing collected packages: kobert\n","Successfully installed kobert-0.1.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xSgXB81At3vD"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtbZxE02t3vD"},"source":["from kobert.utils import get_tokenizer\n","from kobert.pytorch_kobert import get_pytorch_kobert_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0_cnrk-nt3vE"},"source":["from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmyKFEnmt3vE"},"source":["##GPU 사용 시\n","device = torch.device(\"cuda:0\")  # \"cuda:0\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oyG0Ebi1fFgs","executionInfo":{"status":"ok","timestamp":1629942562665,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"52e87127-f7db-4ec4-e70b-d03f17dc2fd6"},"source":["print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTZUhSz9t3vF","executionInfo":{"status":"ok","timestamp":1629942589595,"user_tz":-540,"elapsed":26937,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"5229a5fd-733d-4a41-bf72-87c990f18be8"},"source":["bertmodel, vocab = get_pytorch_kobert_model()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[██████████████████████████████████████████████████]\n","[██████████████████████████████████████████████████]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-d00uR0BPGo","executionInfo":{"status":"ok","timestamp":1629942589596,"user_tz":-540,"elapsed":39,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"717b1f71-1a0b-4b18-c558-09acd9da55c2"},"source":["tokenizer = get_tokenizer()\n","tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["using cached model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RT40pMn1I151"},"source":["class myBertDataset(Dataset):\n","    def __init__(self, dataset, bert_tokenizer, max_len, pad, pair):\n","        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length = max_len, pad=pad, pair= pair)\n","        self.sentences = [transform(dataset[i][0:2]) for i in range(0,len(dataset))]\n","        self.labels = [np.int32(dataset[i][2]) for i in range(0,len(dataset))]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","    \n","    def __len__(self): \n","        return (len(self.labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfd9FnbQKdWG"},"source":["## Setting parameters\n","max_len = 512\n","batch_size = 8\n","warmup_ratio = 0.1\n","num_epochs = 4\n","max_grad_norm = 1\n","log_interval = 200\n","learning_rate =  5e-5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4Cxys_PKnJh"},"source":["data_Train = myBertDataset(train_data, tok, max_len, True, True)\n","data_Test = myBertDataset(test_data, tok, max_len, True, True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV6f9QaP63Mh","executionInfo":{"status":"ok","timestamp":1629942674204,"user_tz":-540,"elapsed":44,"user":{"displayName":"‍조혁준(학부생-빅데이터경영통계전공)","photoUrl":"","userId":"00769196286859152404"}},"outputId":"0159efd3-c1f6-4d57-97b6-fee934bdd534"},"source":["train_dataloader = torch.utils.data.DataLoader(data_Train, batch_size=batch_size, num_workers=5)\n","test_dataloader = torch.utils.data.DataLoader(data_Test, batch_size=batch_size, num_workers=5)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lmaGtq0Rt3vK"},"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert,\n","                 hidden_size = 768, \n","                 num_classes=2,\n","                 dr_rate=None,\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","                 \n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","    \n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1   \n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","        \n","        pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))[0]\n","        if self.dr_rate:\n","            pooler = pooler[:,0,:] \n","            out = self.dropout(pooler)\n","        return self.classifier(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oNwqRaQ_t3vK"},"source":["model = BERTClassifier(bertmodel, dr_rate=0.2).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSKJD20It3vL"},"source":["# Prepare optimizer and schedule (linear warmup and decay)\n","no_decay = ['bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wk1ADJQDt3vL"},"source":["optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n","loss_fn = nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkMR15qtt3vL"},"source":["t_total = len(train_dataloader) * num_epochs\n","warmup_step = int(t_total * warmup_ratio)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gsA92Kt9t3vM"},"source":["scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rblXWCF2t3vM"},"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXWL7owfbHlv"},"source":["OUTPUT_PATH = '/content/gdrive/MyDrive/데청캠 3조 공유 드라이브/Model/성민/output/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4ho7cu3Eg7D"},"source":["class EarlyStopping:\n","    def __init__(self, patience=2, verbose=False, delta=0, path=OUTPUT_PATH+'checkpoint_final.pt'):\n","        self.patience = patience\n","        self.verbose = verbose\n","        self.counter = 0\n","        self.best_score = None\n","        self.early_stop = False\n","        self.val_loss_min = np.Inf\n","        self.delta = delta\n","        self.path = path\n","\n","    def __call__(self, val_loss, model):\n","\n","        score = -val_loss\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(val_loss, model)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, val_loss, model):\n","        if self.verbose:\n","            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n","        torch.save(model.state_dict(), self.path)\n","        self.val_loss_min = val_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5baa7ea8994c4240b9a96f2235790b26","7a7456d9a9a043eaaa94f5795f563852","b6b582a5a74341388f01888f8439778f","304939764b9b482cb198dbe1e5231545","3263bd652d13449aa4a0f5366f2dee3c","552e81718bb744ff9a5eb5209a30f386","ba0ab3f2167947b590e1802931a96589","ecfae73b6eb544b4b858229862dc729e","4beda4125333499fa8650377c03ada16","fd02dbd67f4641ae95f73fe744a9549b","9454470b040d46539a6a1d0b221a1214"]},"id":"LxBlGeB0t3vM","outputId":"2504f1b7-cdd8-49cf-d519-ef116b5c6c10"},"source":["test_losses = []\n","avg_test_losses = []\n","early_stopping = EarlyStopping(patience = 2, verbose = True)\n","for e in range(num_epochs):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    model.train()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        #print(out.shape)\n","        #print(label.shape)\n","        loss = loss_fn(out, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        train_acc += calc_accuracy(out, label)\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","    model.eval()\n","    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(test_dataloader)):\n","        token_ids = token_ids.long().to(device)\n","        segment_ids = segment_ids.long().to(device)\n","        valid_length= valid_length\n","        label = label.long().to(device)\n","        out = model(token_ids, valid_length, segment_ids)\n","        loss = loss_fn(out, label)\n","        test_losses.append(loss.item())\n","        test_acc += calc_accuracy(out, label)\n","    test_loss = np.average(test_losses)\n","    test_losses = []\n","    early_stopping(test_loss, model)\n","\n","    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n","\n","    if early_stopping.early_stop:\n","        print('Early stopping')\n","        break\n","    torch.save(model.state_dict(), OUTPUT_PATH+'checkpoint_epoch'+str(e)+'.pt')"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5baa7ea8994c4240b9a96f2235790b26","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/32567 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","text":["epoch 1 batch id 1 loss 0.6626768112182617 train acc 0.625\n","epoch 1 batch id 201 loss 0.6407122611999512 train acc 0.746268656716418\n","epoch 1 batch id 401 loss 0.4520088732242584 train acc 0.8095386533665836\n","epoch 1 batch id 601 loss 0.1840997189283371 train acc 0.831946755407654\n","epoch 1 batch id 801 loss 0.439775675535202 train acc 0.848314606741573\n","epoch 1 batch id 1001 loss 0.8665885329246521 train acc 0.8577672327672328\n","epoch 1 batch id 1201 loss 0.06059405952692032 train acc 0.8648001665278934\n","epoch 1 batch id 1401 loss 0.04298820346593857 train acc 0.8694682369735903\n","epoch 1 batch id 1601 loss 0.4648098051548004 train acc 0.8735165521549032\n","epoch 1 batch id 1801 loss 0.49559348821640015 train acc 0.8786091060521932\n","epoch 1 batch id 2001 loss 0.02482970617711544 train acc 0.8821214392803598\n","epoch 1 batch id 2201 loss 0.014247480779886246 train acc 0.8834620626987733\n","epoch 1 batch id 2401 loss 0.06834141165018082 train acc 0.8861932528113287\n","epoch 1 batch id 2601 loss 0.006406990811228752 train acc 0.8876874279123415\n","epoch 1 batch id 2801 loss 0.011310720816254616 train acc 0.8900838986076401\n","epoch 1 batch id 3001 loss 0.6705287098884583 train acc 0.8924941686104632\n","epoch 1 batch id 3201 loss 0.009914170950651169 train acc 0.8945251483911277\n","epoch 1 batch id 3401 loss 0.06630592793226242 train acc 0.8961702440458689\n","epoch 1 batch id 3601 loss 0.6693477630615234 train acc 0.897493751735629\n","epoch 1 batch id 3801 loss 0.3399633765220642 train acc 0.8983820047355959\n","epoch 1 batch id 4001 loss 0.010060392320156097 train acc 0.8995563609097725\n","epoch 1 batch id 4201 loss 0.028935249894857407 train acc 0.9010057129254939\n","epoch 1 batch id 4401 loss 0.008163487538695335 train acc 0.9015564644399\n","epoch 1 batch id 4601 loss 0.007237186189740896 train acc 0.9023310149967398\n","epoch 1 batch id 4801 loss 0.003561349818482995 train acc 0.9034055405123933\n","epoch 1 batch id 5001 loss 0.04867561534047127 train acc 0.9043191361727655\n","epoch 1 batch id 5201 loss 0.36213892698287964 train acc 0.9044174197269755\n","epoch 1 batch id 5401 loss 0.017638251185417175 train acc 0.9051564525087946\n","epoch 1 batch id 5601 loss 0.6903901100158691 train acc 0.9057311194429566\n","epoch 1 batch id 5801 loss 0.006891641765832901 train acc 0.9068479572487502\n","epoch 1 batch id 6001 loss 0.0037177912890911102 train acc 0.9075987335444092\n","epoch 1 batch id 6201 loss 0.44445887207984924 train acc 0.908220448314788\n","epoch 1 batch id 6401 loss 0.45745930075645447 train acc 0.9083736916106858\n","epoch 1 batch id 6601 loss 0.25349879264831543 train acc 0.9084797757915467\n","epoch 1 batch id 6801 loss 0.0467551127076149 train acc 0.9088369357447434\n","epoch 1 batch id 7001 loss 0.09698669612407684 train acc 0.9091379802885302\n","epoch 1 batch id 7201 loss 0.017376327887177467 train acc 0.9097000416608805\n","epoch 1 batch id 7401 loss 0.5134449005126953 train acc 0.9098432644237265\n","epoch 1 batch id 7601 loss 0.06912502646446228 train acc 0.9100776213656098\n","epoch 1 batch id 7801 loss 0.19640681147575378 train acc 0.9100435841558775\n","epoch 1 batch id 8001 loss 0.006375400815159082 train acc 0.9102299712535933\n","epoch 1 batch id 8201 loss 0.2671319544315338 train acc 0.91048347762468\n","epoch 1 batch id 8401 loss 0.7012672424316406 train acc 0.9106653969765504\n","epoch 1 batch id 8601 loss 0.026554586365818977 train acc 0.9106789908150215\n","epoch 1 batch id 8801 loss 0.027574803680181503 train acc 0.9107487785478923\n","epoch 1 batch id 9001 loss 0.006593729369342327 train acc 0.9108015776024886\n","epoch 1 batch id 9201 loss 0.06838688254356384 train acc 0.9104309314204978\n","epoch 1 batch id 9401 loss 0.7861064672470093 train acc 0.9105414317625784\n","epoch 1 batch id 9601 loss 0.011449404060840607 train acc 0.9105692115404646\n","epoch 1 batch id 9801 loss 0.4383045434951782 train acc 0.9104555657585961\n","epoch 1 batch id 10001 loss 0.01065535843372345 train acc 0.9105839416058394\n","epoch 1 batch id 10201 loss 0.45192205905914307 train acc 0.9104499558866778\n","epoch 1 batch id 10401 loss 0.02266557514667511 train acc 0.9105254302470916\n","epoch 1 batch id 10601 loss 0.12786878645420074 train acc 0.910586265446656\n","epoch 1 batch id 10801 loss 0.0061805699951946735 train acc 0.9107721507267845\n","epoch 1 batch id 11001 loss 0.03140140324831009 train acc 0.9106103990546314\n","epoch 1 batch id 11201 loss 0.03867822885513306 train acc 0.9105883403267565\n","epoch 1 batch id 11401 loss 0.36756089329719543 train acc 0.9105560915709149\n","epoch 1 batch id 11601 loss 0.32831481099128723 train acc 0.9107512283423843\n","epoch 1 batch id 11801 loss 0.28376325964927673 train acc 0.9106537581560885\n","epoch 1 batch id 12001 loss 0.23239637911319733 train acc 0.910288725939505\n","epoch 1 batch id 12201 loss 0.015401527285575867 train acc 0.9103659536103598\n","epoch 1 batch id 12401 loss 0.025603801012039185 train acc 0.9106221272478026\n","epoch 1 batch id 12601 loss 0.46543630957603455 train acc 0.9103741766526466\n","epoch 1 batch id 12801 loss 0.022696195170283318 train acc 0.9104073900476525\n","epoch 1 batch id 13001 loss 0.8423953652381897 train acc 0.9102761326051843\n","epoch 1 batch id 13201 loss 0.4327254295349121 train acc 0.9100825695023105\n","epoch 1 batch id 13401 loss 0.048382434993982315 train acc 0.9098108350123125\n","epoch 1 batch id 13601 loss 0.462251216173172 train acc 0.9095011396220866\n","epoch 1 batch id 13801 loss 0.28813591599464417 train acc 0.9093543946090863\n","epoch 1 batch id 14001 loss 0.027039237320423126 train acc 0.9090689950717806\n","epoch 1 batch id 14201 loss 0.21578744053840637 train acc 0.9088356453770862\n","epoch 1 batch id 14401 loss 0.6806735992431641 train acc 0.9086608568849386\n","epoch 1 batch id 14601 loss 0.2856312096118927 train acc 0.9084651736182453\n","epoch 1 batch id 14801 loss 1.078014612197876 train acc 0.9083170056077292\n","epoch 1 batch id 15001 loss 0.06634306907653809 train acc 0.9081811212585827\n","epoch 1 batch id 15201 loss 0.7709237933158875 train acc 0.9079830274324058\n","epoch 1 batch id 15401 loss 0.14672601222991943 train acc 0.90782254399065\n","epoch 1 batch id 15601 loss 0.09705547988414764 train acc 0.9073216460483302\n","epoch 1 batch id 15801 loss 0.25823211669921875 train acc 0.9070153787734954\n","epoch 1 batch id 16001 loss 0.08668289333581924 train acc 0.906669895631523\n","epoch 1 batch id 16201 loss 0.05416824296116829 train acc 0.9067032899203753\n","epoch 1 batch id 16401 loss 0.5278412103652954 train acc 0.9063624169257972\n","epoch 1 batch id 16601 loss 0.12574097514152527 train acc 0.9061803505812903\n","epoch 1 batch id 16801 loss 0.8476494550704956 train acc 0.9058910183917624\n","epoch 1 batch id 17001 loss 0.23247213661670685 train acc 0.9056599611787541\n","epoch 1 batch id 17201 loss 0.14790336787700653 train acc 0.9055214813092262\n","epoch 1 batch id 17401 loss 0.25008541345596313 train acc 0.9053574507212229\n","epoch 1 batch id 17601 loss 0.5846907496452332 train acc 0.905019601159025\n","epoch 1 batch id 17801 loss 0.22640925645828247 train acc 0.9047525419920229\n","epoch 1 batch id 18001 loss 0.3836502730846405 train acc 0.9045816899061163\n","epoch 1 batch id 18201 loss 0.4767107367515564 train acc 0.904373386077688\n","epoch 1 batch id 18401 loss 0.1199701726436615 train acc 0.9041492310200533\n","epoch 1 batch id 18601 loss 0.5256748199462891 train acc 0.9036207730767163\n","epoch 1 batch id 18801 loss 0.3124504089355469 train acc 0.9029572895058774\n","epoch 1 batch id 19001 loss 0.1329478770494461 train acc 0.9019656860165255\n","epoch 1 batch id 19201 loss 0.28400689363479614 train acc 0.9007148065204937\n","epoch 1 batch id 19401 loss 0.634250819683075 train acc 0.9003659605174991\n","epoch 1 batch id 19601 loss 0.20423853397369385 train acc 0.8996989949492373\n","epoch 1 batch id 19801 loss 0.0840805396437645 train acc 0.8988561183778597\n","epoch 1 batch id 20001 loss 0.29237890243530273 train acc 0.8982050897455127\n","epoch 1 batch id 20201 loss 1.0684984922409058 train acc 0.8976721449433196\n","epoch 1 batch id 20401 loss 0.6519973874092102 train acc 0.8971741581295034\n","epoch 1 batch id 20601 loss 0.303690105676651 train acc 0.8966615698267074\n","epoch 1 batch id 20801 loss 0.35796818137168884 train acc 0.8959665400701889\n","epoch 1 batch id 21001 loss 0.15369950234889984 train acc 0.895326413027951\n","epoch 1 batch id 21201 loss 0.12016358971595764 train acc 0.8948811376821848\n","epoch 1 batch id 21401 loss 0.3152318000793457 train acc 0.8940995747862249\n","epoch 1 batch id 21601 loss 0.2679598033428192 train acc 0.8935813156798297\n","epoch 1 batch id 21801 loss 0.400244802236557 train acc 0.893049630750883\n","epoch 1 batch id 22001 loss 0.6273872256278992 train acc 0.8917549202308986\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YA77-U1JE7QU"},"source":["model = model.load_state_dict(torch.load('checkpoint.pt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rTahJ98YHfIG"},"source":[""],"execution_count":null,"outputs":[]}]}